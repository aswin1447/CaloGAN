\section{Our GAN model} \label{sec:model}
Our idea is to treat simulations as a black-box and replace the traditional Monte Carlo simulation with a method based on Generative Adversarial Networks. As WGANs with gradient penalty are considered to be the state-of-the-art technique for image producing, we implement a tool based on this approach. For it to be useful in realistic physics applications, such a system needs to be able to accept requests for the generation of showers originating from incoming particle parameters such as 3d momentum and 2d coordinate. We introduce an auxiliary task of reconstructing these parameters $p_x$, $p_y$, $p_z$ and $x$, $y$ from a shower image.

\subsection{Model architecture}

We need to generate a specific calorimeter response for a particle with some parameters. It means that the model is required to be conditional.
% sampling not just from $p(\textbf{y}),$ but from $p(\textbf{y}|\vx),$ so, 
Firstly, we describe a generator and discriminator architecture. The generator maps from an input (a 512 $\times$ 1 vector sampled from a Gaussian distribution and the particle parameters) to a 30 $\times$ 30 image $\hat{\textbf{y}}$ using deconvolutional layers (in fact, it is an upsampling procedure and convolutions) which are arranged as follows. We concatenate the noise vector and the parameters $(p_x,~ p_y,~ p_z,~ x,~ y)$, after which we add a fully connected layer with reshaping and obtain a 256 $\times$ 4 $\times$ 4 output. After a sequence of 2d deconvolutions, we get outputs of size  128 $\times$ 8 $\times$ 8, 64 $\times$ 15 $\times$ 16 and 32 $\times$ 32 $\times$ 32  with ReLu activation functions. After this procedure, we crop the last output to obtain the image of the desired size 30 $\times$ 30.

As for the discriminator, it takes a batch of images as input (all images in the batch are real or generated by $G$) and returns the score $D(\textbf{y})$ or $D(\hat{\textbf{y}})$ as it is described in \cite{arjovsky2017wasserstein}. The discriminator architecture is simply the reversed generator architecture (i.e. sizes of layers go in the opposite order). It implies that we have a 30 $\times$ 30 matrix as input, from which we obtain output layers of size 32 $\times$ 32 $\times$ 32, 64 $\times$ 15 $\times$ 16, 128 $\times$ 8 $\times$  8, followed by reshaping, which  leads to 256 $\times$ 4 $\times$ 4, and by applying LeakyRelu activation function we get the final score. The model scheme is presented in~\cref{fig:model}.

\begin{figure}
\centering
\includegraphics[width=0.95\textwidth]{figures/model_architecture.pdf}
\caption{Model architecture. Pre-trained regressor for the particle parameters prediction makes our model conditional. Thanks to building up the information from the pre-trained regressor into the discriminator gradient we learn $G$ to produce a specific calorimeter response.}\label{fig:model}
\end{figure}

How to train WGAN with gradient penalty in a conditional manner is described in the following section.

\subsection{Training strategy} \label{sec:training_strategy}
Due to the nature of WGAN loss, conditioning on the continuous value is a non-trivial task. To overcome this issue we suggest embedding a pre-trained regressor in our model. We train a neural network to predict the particle parameters by the calorimeter response. As for architecture, it has the same one as the discriminator but with a perceptual loss described in \cite{johnson2016perceptual}, because it was seen to work better compared to standard MSE. By building up the information from the pre-trained regressor into the discriminator gradient, we obtain the conditional model because we train the generator and the discriminator together. As a result, the discriminator makes the generator produce a specific calorimeter response.

Matrices from our dataset are pretty sparse because almost all information is located in central cells (see~\cref{fig:real-imgs}). To make the optimization process easier we apply a box-cox transformation. This mapping helps to smooth the data that makes the optimization process more stable.
Results obtained with the described model are presented in the following section.

%PyTorch \cite{pytorch} library was used to carry out model training and experiments.

\begin{figure}
\begin{center}
\includegraphics[width=0.3\textwidth]{figures/mean_cluster.pdf}
\caption{energy deposition in different cells of used 30$\times $30 setup for \geant simulated events averaged over all events in the used dataset. \label{fig:real-imgs}}
\end{center}
\end{figure}
